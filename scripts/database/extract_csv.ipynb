{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed, 1000 documents done.\n",
      "Batch 2 processed, 2000 documents done.\n",
      "Batch 3 processed, 3000 documents done.\n",
      "Batch 4 processed, 4000 documents done.\n",
      "Batch 5 processed, 5000 documents done.\n",
      "Batch 6 processed, 6000 documents done.\n",
      "Batch 7 processed, 7000 documents done.\n",
      "Batch 8 processed, 8000 documents done.\n",
      "Batch 9 processed, 9000 documents done.\n",
      "Batch 10 processed, 10000 documents done.\n",
      "Batch 11 processed, 11000 documents done.\n",
      "Batch 12 processed, 12000 documents done.\n",
      "Batch 13 processed, 13000 documents done.\n",
      "Batch 14 processed, 14000 documents done.\n",
      "Batch 15 processed, 15000 documents done.\n",
      "Batch 16 processed, 16000 documents done.\n",
      "Batch 17 processed, 17000 documents done.\n",
      "Batch 18 processed, 18000 documents done.\n",
      "Batch 19 processed, 19000 documents done.\n",
      "Batch 20 processed, 20000 documents done.\n",
      "Batch 21 processed, 21000 documents done.\n",
      "Batch 22 processed, 22000 documents done.\n",
      "Batch 23 processed, 23000 documents done.\n",
      "Batch 24 processed, 24000 documents done.\n",
      "Batch 25 processed, 25000 documents done.\n",
      "Batch 26 processed, 26000 documents done.\n",
      "Batch 27 processed, 27000 documents done.\n",
      "Batch 28 processed, 28000 documents done.\n",
      "Batch 29 processed, 29000 documents done.\n",
      "Batch 30 processed, 30000 documents done.\n",
      "Batch 31 processed, 31000 documents done.\n",
      "Batch 32 processed, 32000 documents done.\n",
      "Batch 33 processed, 33000 documents done.\n",
      "Batch 34 processed, 34000 documents done.\n",
      "Batch 35 processed, 35000 documents done.\n",
      "Batch 36 processed, 36000 documents done.\n",
      "Batch 37 processed, 37000 documents done.\n",
      "Batch 38 processed, 38000 documents done.\n",
      "Batch 39 processed, 39000 documents done.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "MONGO_URI = 'mongodb://datasci:scopus888@datascidb.kanakornmek.dev:27017/?authSource=admin'\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_URI)\n",
    "\n",
    "db = client['datasci']\n",
    "collection = db['papers']\n",
    "\n",
    "data = []\n",
    "\n",
    "batch_size = 1000\n",
    "batch_count = 0\n",
    "document_count = 0\n",
    "\n",
    "cursor = collection.find().batch_size(batch_size)\n",
    "for doc in cursor:\n",
    "    title = doc['coredata'].get('dc:title')\n",
    "    abstract = None\n",
    "    if 'item' in doc and 'bibrecord' in doc['item'] and 'head' in doc['item']['bibrecord']:\n",
    "        abstract = doc['item']['bibrecord']['head'].get('abstracts')\n",
    "    else:\n",
    "        abstract = doc['coredata']['dc:description']\n",
    "    authkeywords = None\n",
    "    if 'authkeywords' in doc and doc['authkeywords']:\n",
    "        if isinstance(doc['authkeywords']['author-keyword'], list):\n",
    "            authkeywords = \" \".join([keyword['$'] for keyword in doc['authkeywords']['author-keyword']])\n",
    "        else:\n",
    "            authkeywords = [doc['authkeywords']['author-keyword']['$']] if doc['authkeywords']['author-keyword'] else None\n",
    "    category = doc['subject-areas']['subject-area'][0].get('@abbrev') if 'subject-areas' in doc else None\n",
    "    data.append({'title': title, 'abstract': abstract, 'authkeywords': authkeywords, 'category': category})\n",
    "    \n",
    "    document_count += 1\n",
    "    \n",
    "    if document_count % batch_size == 0:\n",
    "        batch_count += 1\n",
    "        print(f\"Batch {batch_count} processed, {document_count} documents done.\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "filename = os.path.join(os.getenv('USERPROFILE'), 'Documents', 'data.csv')\n",
    "df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39190, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Combine and preprocess text fields\n",
    "texts = df['title'].fillna('') + \" \" + df['abstract'].fillna('') + \" \" + df['authkeywords'].fillna('').apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "texts = texts.apply(preprocess_text)\n",
    "\n",
    "df['texts'] = texts\n",
    "filename = os.path.join(os.getenv('USERPROFILE'), 'Documents', 'data_preprocessed_1.csv')\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Public health and international epidemiology f...   \n",
      "1  Flexible Printed Active Antenna for Digital Te...   \n",
      "2  Parametric study of hydrogen production via so...   \n",
      "3  Superhydrophobic coating from fluoroalkylsilan...   \n",
      "4  Electrochemical impedance-based DNA sensor usi...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0                                               None   \n",
      "1  © 2018 The Institute of Electronics, Informati...   \n",
      "2  © 2018 Elsevier LtdComputational fluid dynamic...   \n",
      "3  © 2018 Elsevier B.V. A superhydrophobic/supero...   \n",
      "4  © 2018 Elsevier B.V. A label-free electrochemi...   \n",
      "\n",
      "                                        authkeywords category  \n",
      "0                                               None     MEDI  \n",
      "1                                               None     ENGI  \n",
      "2  Circulating fluidized bed Computational fluid ...     CHEM  \n",
      "3  Encapsulation Fluoroalkylsilane Natural rubber...     CHEM  \n",
      "4  acpcPNA Electrochemical impedance spectroscopy...     CHEM  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
